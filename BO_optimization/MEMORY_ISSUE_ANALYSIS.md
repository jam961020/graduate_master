# ë©”ëª¨ë¦¬/í”„ë¡œì„¸ìŠ¤ ì…§ë‹¤ìš´ ë¬¸ì œ ë¶„ì„ ë° í•´ê²° (2025-11-13)

## ğŸ“Š ë¬¸ì œ ìƒí™©

### ì‹¤í—˜ ê²°ê³¼ íŒ¨í„´
| ì‹œë„ | ë©”ëª¨ë¦¬ ê´€ë¦¬ ë°©ë²• | ë©ˆì¶˜ ì§€ì  | ë¹„ê³  |
|------|------------------|-----------|------|
| Trial 1 | ê¸°ë³¸ (iteration ëë§Œ) | **13ë²ˆ** | ì²« ë²ˆì§¸ ë²½ |
| Trial 2 | Iteration ë í•´ì œ ê°•í™” | **36ë²ˆ** | 2.7ë°° ê°œì„  âœ¨ |
| Trial 3 | GP 5ë²ˆë§ˆë‹¤ + ê³¼ë„í•œ í•´ì œ | **6ë²ˆ** | ì˜¤íˆë ¤ ì•…í™” âŒ |
| Trial 4 | OpenCV í•´ì œ ì¶”ê°€ | **13ë²ˆ** | ì²« ë²ˆì§¸ ë²½ ì¬ë°œ |

### ê³µí†µ ì¦ìƒ
- **ì—ëŸ¬ ë©”ì‹œì§€ ì—†ì´ ì¡°ìš©íˆ ì¢…ë£Œ** (exit code ì—†ìŒ)
- **13ë²ˆ ë˜ëŠ” 36ë²ˆì—ì„œ ì¼ê´€ë˜ê²Œ ë©ˆì¶¤**
- Windows Git Bash í™˜ê²½

---

## ğŸ” ì›ì¸ ë¶„ì„ (Opus)

### 1. GPU ë©”ëª¨ë¦¬ ì˜¤ë²„í”Œë¡œìš° âš ï¸
**ë¬¸ì œì **:
- AirLine ëª¨ë¸ë“¤(DexiNed, OrientationDetector)ì´ GPUì— ìƒì£¼
- ë§¤ ì´ë¯¸ì§€ë§ˆë‹¤ GPU ì—°ì‚° ëˆ„ì 
- `torch.cuda.empty_cache()` í˜¸ì¶œ ë¶€ì¡±

**ê·¼ê±°**:
- Trial 2ì—ì„œ iteration ë ë©”ëª¨ë¦¬ í•´ì œ ê°•í™” â†’ 36ë²ˆê¹Œì§€ ê°œì„ 
- GPU ë©”ëª¨ë¦¬ê°€ ìŒ“ì´ë‹¤ê°€ ì„ê³„ì ì—ì„œ í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ

### 2. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íŒ¨í„´ âš ï¸
**ë¬¸ì œì **:
- `AirLine_assemble_test.py`ì—ì„œ **ì „ì—­ ë²„í¼ ì‚¬ìš©** (TMP1, TMP2, TMP3)
- ì´ë¯¸ì§€ ì¦ê°• ì‹œ **float32 ì‚¬ìš©**ìœ¼ë¡œ ë©”ëª¨ë¦¬ ê³¼ë‹¤
- 119ì¥ ì´ë¯¸ì§€ë¥¼ **ì „ì²´ ë©”ëª¨ë¦¬ì— ìœ ì§€**

**ê·¼ê±°**:
- OpenCV ë©”ëª¨ë¦¬ í•´ì œ ì¶”ê°€í–ˆëŠ”ë°ë„ 13ë²ˆì—ì„œ ë©ˆì¶¤
- AirLine ë‚´ë¶€ì˜ ì „ì—­ ë³€ìˆ˜ê°€ ê³„ì† ëˆ„ì 

### 3. C++ ëª¨ë“ˆ ë¬¸ì œ (ê°€ëŠ¥ì„±)
**ë¬¸ì œì **:
- `CRG311.pyd` C++ í™•ì¥ ëª¨ë“ˆì˜ ë©”ëª¨ë¦¬ ê´€ë¦¬ ë¶ˆëª…í™•
- ì„¸ê·¸ë©˜í…Œì´ì…˜ í´íŠ¸ ê°€ëŠ¥ì„±

### 4. ë¦¬ì†ŒìŠ¤ ì œí•œ
**ë¬¸ì œì **:
- íŒŒì¼ ë””ìŠ¤í¬ë¦½í„° í•œê³„ ë„ë‹¬ ê°€ëŠ¥ì„±
- ìŠ¤ë ˆë“œ ìˆ˜ ê³¼ë‹¤ ìƒì„±

---

## âœ… ì œì•ˆëœ í•´ê²°ì±…

### ìš°ì„ ìˆœìœ„ 1: GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°•í™” (ì¦‰ì‹œ ì ìš© ê°€ëŠ¥)

#### A. GPU ë©”ëª¨ë¦¬ 80% ì œí•œ ì„¤ì •
```python
import torch
import os

# optimization.py ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€
if torch.cuda.is_available():
    torch.cuda.set_per_process_memory_fraction(0.8)
    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:128'
```

#### B. ì£¼ê¸°ì  GPU ìºì‹œ ì •ë¦¬
```python
# BO iteration ë£¨í”„ì—ì„œ
for iteration in range(n_iterations):
    # ... ê¸°ì¡´ ì½”ë“œ ...

    # ë§¤ iteration ë
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()  # GPU ì—°ì‚° ì™„ë£Œ ëŒ€ê¸°
    import gc
    gc.collect()

    # 5ë²ˆë§ˆë‹¤ ë” ê°•ë ¥í•œ ì •ë¦¬
    if (iteration + 1) % 5 == 0:
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            torch.cuda.reset_peak_memory_stats()
        gc.collect()
        gc.collect()  # ìˆœí™˜ ì°¸ì¡° ì •ë¦¬
```

---

### ìš°ì„ ìˆœìœ„ 2: ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ (ì¤‘ìš”!)

**ëª©í‘œ**: í”„ë¡œì„¸ìŠ¤ê°€ í„°ì ¸ë„ ì¤‘ê°„ ê²°ê³¼ ë³´ì¡´ ë° ì¬ì‹œì‘ ê°€ëŠ¥

#### ì²´í¬í¬ì¸íŠ¸ ì €ì¥
```python
import json
from pathlib import Path

def save_checkpoint(iteration, train_X_full, train_Y, best_cvar_history,
                    best_params, checkpoint_dir):
    """5ë²ˆë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥"""
    checkpoint = {
        'iteration': iteration,
        'train_X_full': train_X_full.cpu().numpy().tolist(),
        'train_Y': train_Y.cpu().numpy().tolist(),
        'best_cvar_history': best_cvar_history,
        'best_params': best_params,
    }

    checkpoint_file = checkpoint_dir / f"checkpoint_iter_{iteration:03d}.json"
    with open(checkpoint_file, 'w') as f:
        json.dump(checkpoint, f, indent=2)

    print(f"  [Checkpoint] Saved at iteration {iteration}")

# BO ë£¨í”„ì—ì„œ
for iteration in range(n_iterations):
    # ... ê¸°ì¡´ ì½”ë“œ ...

    # 5ë²ˆë§ˆë‹¤ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
    if (iteration + 1) % 5 == 0:
        save_checkpoint(iteration + 1, train_X_full, train_Y,
                       best_cvar_history, best_params, checkpoint_dir)
```

#### ì²´í¬í¬ì¸íŠ¸ì—ì„œ ì¬ì‹œì‘
```python
def load_checkpoint(checkpoint_dir):
    """ìµœì‹  ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ"""
    checkpoint_files = list(checkpoint_dir.glob("checkpoint_iter_*.json"))
    if not checkpoint_files:
        return None

    latest = max(checkpoint_files, key=lambda p: p.stat().st_mtime)
    with open(latest) as f:
        checkpoint = json.load(f)

    print(f"  [Checkpoint] Loaded from {latest.name}")
    return checkpoint
```

---

### ìš°ì„ ìˆœìœ„ 3: AirLine ë©”ëª¨ë¦¬ ê´€ë¦¬

#### full_pipeline.py ê°œì„ 
```python
def detect_with_full_pipeline(image, params, yolo_detector, ransac_weights=None):
    # ... ê¸°ì¡´ ì½”ë“œ ...

    # ROI ë£¨í”„ ë‚´ë¶€
    for cls, x1_roi, y1_roi, x2_roi, y2_roi in rois:
        # ... ì²˜ë¦¬ ...

        # âœ… ê° ROI ì²˜ë¦¬ í›„ ì¦‰ì‹œ ë©”ëª¨ë¦¬ í•´ì œ
        del roi_bgr, roi_gray, roi_gray_blur, S_roi
        del roi_bgr_enhanced, roi_gray_enhanced, lines_by_algo

    # âœ… í•¨ìˆ˜ ì¢…ë£Œ ì „ ìµœì¢… ì •ë¦¬
    del processed_results, rois

    # âœ… GPU ì‚¬ìš©í–ˆë‹¤ë©´ ì •ë¦¬
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    import gc
    gc.collect()

    return coords
```

---

### ìš°ì„ ìˆœìœ„ 4: ì§„ë‹¨ ë„êµ¬ (ì„ íƒì‚¬í•­)

#### ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§
```python
import psutil
import torch

def log_memory_usage(iteration):
    """ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë¡œê¹…"""
    process = psutil.Process()
    mem_info = process.memory_info()

    print(f"  [Memory] Iter {iteration}: "
          f"RAM={mem_info.rss / 1024**2:.1f}MB, "
          f"GPU={torch.cuda.memory_allocated() / 1024**2:.1f}MB")

# BO ë£¨í”„ì—ì„œ ì‚¬ìš©
for iteration in range(n_iterations):
    # ... ê¸°ì¡´ ì½”ë“œ ...

    if (iteration + 1) % 5 == 0:
        log_memory_usage(iteration + 1)
```

---

## ğŸ¯ ì ìš© ê³„íš (ë‹¨ê³„ë³„)

### Phase 1: ì¦‰ì‹œ ì ìš© (ê°€ì¥ ì¤‘ìš”!)
1. âœ… **GPU ë©”ëª¨ë¦¬ 80% ì œí•œ** - ì‹œì‘ ë¶€ë¶„ì— ì¶”ê°€
2. âœ… **ì£¼ê¸°ì  GPU ìºì‹œ ì •ë¦¬** - iteration ëê³¼ 5ë²ˆë§ˆë‹¤
3. âœ… **ì²´í¬í¬ì¸íŠ¸ ì‹œìŠ¤í…œ** - 5ë²ˆë§ˆë‹¤ ì €ì¥

**ì˜ˆìƒ íš¨ê³¼**: 13ë²ˆ â†’ 30ë²ˆ ì´ìƒ ê¸°ëŒ€

### Phase 2: ì¶”ê°€ ê°œì„  (ì‹œê°„ ìˆìœ¼ë©´)
4. â³ **ë©”ëª¨ë¦¬ ëª¨ë‹ˆí„°ë§ ë¡œê·¸** - ë¬¸ì œ ì§€ì  íŒŒì•…
5. â³ **AirLine ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°•í™”** - ì „ì—­ ë²„í¼ ì •ë¦¬

### Phase 3: ìµœí›„ì˜ ìˆ˜ë‹¨
6. âš ï¸ **n_w ì¤„ì´ê¸°**: 3 â†’ 2 (GP ì°¨ì› ê°ì†Œ)
7. âš ï¸ **ì´ë¯¸ì§€ ìˆ˜ ì¤„ì´ê¸°**: 119ì¥ â†’ 50ì¥
8. âš ï¸ **ë°°ì¹˜ ì²˜ë¦¬**: ì´ë¯¸ì§€ë¥¼ ë‚˜ëˆ ì„œ ì²˜ë¦¬

---

## ğŸ“ Trial 2 ì„±ê³µ ìš”ì¸ ë¶„ì„

**Trial 2ì—ì„œ 36ë²ˆê¹Œì§€ ê°„ ì´ìœ **:
```python
# optimization.py (Trial 2 ì½”ë“œ)
# 5.11: ë©”ëª¨ë¦¬ ëª…ì‹œì  í•´ì œ (13ë²ˆ iteration ë¬¸ì œ í•´ê²°)
if torch.cuda.is_available():
    torch.cuda.empty_cache()
import gc
gc.collect()
```

**í•µì‹¬**:
- **ê°„ë‹¨í•˜ì§€ë§Œ ì¼ê´€ëœ ë©”ëª¨ë¦¬ í•´ì œ**
- ê³¼ë„í•œ `del` ëª…ë ¹ ì—†ì´ ê¸°ë³¸ë§Œ ì¶©ì‹¤íˆ

**êµí›ˆ**:
- ë³µì¡í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ë³´ë‹¤ **ê¸°ë³¸ì— ì¶©ì‹¤**
- **GPU ìºì‹œ ì •ë¦¬**ê°€ ê°€ì¥ ì¤‘ìš”
- `del` ëª…ë ¹ ë‚¨ë°œì€ ì˜¤íˆë ¤ ë¶ˆì•ˆì •

---

## ğŸš€ ë‹¤ìŒ ì‹¤í—˜ ì „ëµ

### ì „ëµ A: Trial 2 ì½”ë“œ ê¸°ë°˜ + GPU ê°•í™”
```python
# Trial 2ì˜ ê°„ë‹¨í•œ ë©”ëª¨ë¦¬ í•´ì œ
+ GPU ë©”ëª¨ë¦¬ 80% ì œí•œ
+ GPU synchronize ì¶”ê°€
+ ì²´í¬í¬ì¸íŠ¸ 5ë²ˆë§ˆë‹¤
```

**ëª©í‘œ**: 36ë²ˆ â†’ 50ë²ˆ

### ì „ëµ B: ë” ë³´ìˆ˜ì  ì ‘ê·¼
```python
# ìœ„ ì „ëµ A
+ n_w = 3 â†’ 2
+ iterations = 50 â†’ 30 (ë” ì§§ê²Œ)
```

**ëª©í‘œ**: ì¼ë‹¨ ì™„ì£¼ ë³´ì¥

---

## ğŸ’¡ í•µì‹¬ ì¸ì‚¬ì´íŠ¸

1. **GPU ë©”ëª¨ë¦¬ê°€ ì£¼ë²”**: AirLine ëª¨ë¸ë“¤ì´ GPUì— ìƒì£¼í•˜ë©° ëˆ„ì 
2. **ê°„ë‹¨í•¨ì´ ìµœê³ **: ë³µì¡í•œ ë©”ëª¨ë¦¬ ê´€ë¦¬ëŠ” ì˜¤íˆë ¤ ë¶ˆì•ˆì •
3. **ì²´í¬í¬ì¸íŠ¸ í•„ìˆ˜**: ì–¸ì œ í„°ì§ˆì§€ ëª¨ë¥´ë‹ˆ ì¤‘ê°„ ì €ì¥ í•„ìˆ˜
4. **13ë²ˆê³¼ 36ë²ˆ**: ì¼ê´€ëœ íŒ¨í„´ = ë©”ëª¨ë¦¬ í•œê³„ì 

---

**ì‘ì„±ì¼**: 2025-11-13
**ìƒíƒœ**: Phase 1 ì ìš© ëŒ€ê¸° ì¤‘
**ë‹¤ìŒ**: GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ + ì²´í¬í¬ì¸íŠ¸ ì ìš© â†’ ì¬ì‹¤í—˜
