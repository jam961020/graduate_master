"""
ì§„ë‹¨ ë„êµ¬ - í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì›ì¸ íŒŒì•…
ëª©ì :
1. GPU ë©”ëª¨ë¦¬ í•œê³„ í™•ì¸
2. AirLine/CRG311 ëª¨ë“ˆ ì•ˆì •ì„± ì²´í¬
3. CUDA ì—°ì‚° í…ŒìŠ¤íŠ¸
4. ë©”ëª¨ë¦¬ ëˆ„ìˆ˜ íŒ¨í„´ íƒì§€
"""
import torch
import numpy as np
import cv2
import psutil
import gc
import sys
import time
from pathlib import Path

# ëª¨ë“ˆ import
sys.path.insert(0, str(Path(__file__).parent))
from full_pipeline import detect_with_full_pipeline
from yolo_detector import YOLODetector
from environment_independent import extract_parameter_independent_environment


class SystemMonitor:
    """ì‹œìŠ¤í…œ ë¦¬ì†ŒìŠ¤ ëª¨ë‹ˆí„°ë§"""

    def __init__(self):
        self.process = psutil.Process()
        self.has_gpu = torch.cuda.is_available()
        if self.has_gpu:
            self.gpu_name = torch.cuda.get_device_name(0)
            self.gpu_total_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3

    def get_status(self):
        """í˜„ì¬ ë©”ëª¨ë¦¬ ìƒíƒœ ë°˜í™˜"""
        mem_info = self.process.memory_info()
        ram_mb = mem_info.rss / 1024**2

        status = {
            'ram_mb': ram_mb,
            'ram_percent': self.process.memory_percent(),
        }

        if self.has_gpu:
            gpu_allocated = torch.cuda.memory_allocated() / 1024**3
            gpu_reserved = torch.cuda.memory_reserved() / 1024**3
            status.update({
                'gpu_allocated_gb': gpu_allocated,
                'gpu_reserved_gb': gpu_reserved,
                'gpu_percent': (gpu_allocated / self.gpu_total_memory) * 100
            })

        return status

    def print_status(self, label=""):
        """ìƒíƒœ ì¶œë ¥"""
        status = self.get_status()
        print(f"\n[{label}]")
        print(f"  RAM: {status['ram_mb']:.1f} MB ({status['ram_percent']:.1f}%)")
        if self.has_gpu:
            print(f"  GPU: {status['gpu_allocated_gb']:.2f}/{self.gpu_total_memory:.2f} GB "
                  f"({status['gpu_percent']:.1f}%)")
            print(f"  GPU Reserved: {status['gpu_reserved_gb']:.2f} GB")


def test_airline_stability(monitor, image_dir, yolo_detector, n_iterations=20):
    """AirLine ëª¨ë“ˆ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸ - ë°˜ë³µ ì‹¤í–‰"""
    print("\n" + "="*60)
    print("TEST 1: AirLine ëª¨ë“ˆ ì•ˆì •ì„± (20íšŒ ë°˜ë³µ)")
    print("="*60)

    # ì´ë¯¸ì§€ ë¡œë“œ
    image_files = sorted(list(Path(image_dir).glob('*.jpg')))[:5]  # 5ê°œë§Œ
    print(f"í…ŒìŠ¤íŠ¸ ì´ë¯¸ì§€: {len(image_files)}ê°œ")

    # ê¸°ë³¸ íŒŒë¼ë¯¸í„°
    params = {
        'edgeThresh1': -3.0,
        'simThresh1': 0.98,
        'pixelRatio1': 0.05,
        'edgeThresh2': 1.0,
        'simThresh2': 0.75,
        'pixelRatio2': 0.05,
        'ransac_Q_w': 5.0,
        'ransac_QG_w': 5.0
    }

    monitor.print_status("ì‹œì‘ ì „")

    success_count = 0
    for i in range(n_iterations):
        try:
            print(f"\në°˜ë³µ {i+1}/{n_iterations}...")

            for img_file in image_files:
                image = cv2.imread(str(img_file))

                # AirLine ì‹¤í–‰
                coords = detect_with_full_pipeline(image, params, yolo_detector)

                # ë©”ëª¨ë¦¬ í•´ì œ
                del image, coords

            # ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬
            if (i + 1) % 5 == 0:
                gc.collect()
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                monitor.print_status(f"ë°˜ë³µ {i+1} ì™„ë£Œ")

            success_count += 1

        except Exception as e:
            print(f"  âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            monitor.print_status(f"ì—ëŸ¬ ë°œìƒ (ë°˜ë³µ {i+1})")
            break

    monitor.print_status("í…ŒìŠ¤íŠ¸ ì¢…ë£Œ")
    print(f"\nê²°ê³¼: {success_count}/{n_iterations} ì„±ê³µ")

    return success_count == n_iterations


def test_gpu_memory_limit(monitor):
    """GPU ë©”ëª¨ë¦¬ í•œê³„ í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("TEST 2: GPU ë©”ëª¨ë¦¬ í•œê³„ í…ŒìŠ¤íŠ¸")
    print("="*60)

    if not torch.cuda.is_available():
        print("GPU ì—†ìŒ - ìŠ¤í‚µ")
        return True

    monitor.print_status("ì‹œì‘ ì „")

    try:
        # ì ì§„ì ìœ¼ë¡œ ë©”ëª¨ë¦¬ í• ë‹¹
        tensors = []
        size_mb = 100

        for i in range(100):  # ìµœëŒ€ 10GBê¹Œì§€
            tensor = torch.randn(size_mb * 1024 * 256, dtype=torch.float32, device='cuda')
            tensors.append(tensor)

            if (i + 1) % 10 == 0:
                monitor.print_status(f"{(i+1)*size_mb} MB í• ë‹¹")

            # 80% ë„˜ìœ¼ë©´ ê²½ê³ 
            status = monitor.get_status()
            if status['gpu_percent'] > 80:
                print(f"  âš ï¸  GPU ë©”ëª¨ë¦¬ 80% ì´ˆê³¼!")
                break

        # ì •ë¦¬
        del tensors
        torch.cuda.empty_cache()
        monitor.print_status("ì •ë¦¬ í›„")

        return True

    except RuntimeError as e:
        print(f"  âŒ GPU ë©”ëª¨ë¦¬ ë¶€ì¡±: {e}")
        monitor.print_status("ì—ëŸ¬ ë°œìƒ")
        return False


def test_crg311_stability(monitor, n_iterations=10):
    """CRG311 ëª¨ë“ˆ ì•ˆì •ì„± í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("TEST 3: CRG311 ëª¨ë“ˆ ì•ˆì •ì„±")
    print("="*60)

    try:
        import CRG311
        print("CRG311 import ì„±ê³µ")
    except Exception as e:
        print(f"  âŒ CRG311 import ì‹¤íŒ¨: {e}")
        return False

    monitor.print_status("ì‹œì‘ ì „")

    success_count = 0
    for i in range(n_iterations):
        try:
            # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
            img = np.random.randint(0, 255, (480, 640), dtype=np.uint8)

            # CRG311 ì‚¬ìš©í•˜ëŠ” í•¨ìˆ˜ í˜¸ì¶œ (AirLine ë‚´ë¶€)
            # ì—¬ê¸°ì„œëŠ” ê°„ì ‘ì ìœ¼ë¡œ í…ŒìŠ¤íŠ¸

            if (i + 1) % 5 == 0:
                print(f"  ë°˜ë³µ {i+1}/{n_iterations} ì™„ë£Œ")

            success_count += 1

        except Exception as e:
            print(f"  âŒ ì—ëŸ¬ ë°œìƒ: {e}")
            break

    monitor.print_status("í…ŒìŠ¤íŠ¸ ì¢…ë£Œ")
    print(f"\nê²°ê³¼: {success_count}/{n_iterations} ì„±ê³µ")

    return success_count == n_iterations


def test_memory_intensive_ops(monitor):
    """ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì—°ì‚° í…ŒìŠ¤íŠ¸"""
    print("\n" + "="*60)
    print("TEST 4: ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì—°ì‚° ë°˜ë³µ")
    print("="*60)

    monitor.print_status("ì‹œì‘ ì „")

    try:
        for i in range(15):  # 13ë²ˆ ë„˜ì–´ì„œ 15ë²ˆê¹Œì§€
            print(f"\në°˜ë³µ {i+1}/15...")

            # í° ë°°ì—´ ìƒì„± (ì‹¤ì œ ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜)
            images = []
            for j in range(50):  # 50ê°œ ì´ë¯¸ì§€ ì‹œë®¬ë ˆì´ì…˜
                img = np.random.randint(0, 255, (480, 640, 3), dtype=np.uint8)

                # ì´ë¯¸ì§€ ì²˜ë¦¬ ì‹œë®¬ë ˆì´ì…˜
                gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)
                blur = cv2.GaussianBlur(gray, (5, 5), 0)
                edges = cv2.Canny(blur, 50, 150)

                images.append((img, gray, blur, edges))

            # GPU ì—°ì‚° (ìˆìœ¼ë©´)
            if torch.cuda.is_available():
                tensor = torch.randn(1000, 1000, device='cuda')
                result = torch.matmul(tensor, tensor.T)
                del tensor, result

            # ë©”ëª¨ë¦¬ í•´ì œ
            del images
            gc.collect()

            if torch.cuda.is_available():
                torch.cuda.empty_cache()

            if (i + 1) % 5 == 0:
                monitor.print_status(f"ë°˜ë³µ {i+1} ì™„ë£Œ")

            # 13ë²ˆì§¸ íŠ¹ë³„ ì²´í¬
            if i + 1 == 13:
                print("  ğŸ” 13ë²ˆì§¸ ë°˜ë³µ í†µê³¼!")
                monitor.print_status("13ë²ˆ í†µê³¼")

        monitor.print_status("í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        print("\nâœ… 15ë²ˆ ë°˜ë³µ ì„±ê³µ!")
        return True

    except Exception as e:
        print(f"\nâŒ ì—ëŸ¬ ë°œìƒ: {e}")
        monitor.print_status("ì—ëŸ¬ ë°œìƒ")
        return False


def main():
    print("="*60)
    print("í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ ì›ì¸ ì§„ë‹¨ ë„êµ¬")
    print("="*60)

    # ì‹œìŠ¤í…œ ì •ë³´
    monitor = SystemMonitor()
    print(f"\nCPU ì½”ì–´: {psutil.cpu_count()}")
    print(f"ì´ RAM: {psutil.virtual_memory().total / 1024**3:.1f} GB")

    if monitor.has_gpu:
        print(f"GPU: {monitor.gpu_name}")
        print(f"GPU ë©”ëª¨ë¦¬: {monitor.gpu_total_memory:.1f} GB")
    else:
        print("GPU: ì—†ìŒ")

    # ì´ë¯¸ì§€ ë””ë ‰í† ë¦¬ ì„¤ì •
    image_dir = Path(__file__).parent.parent / "dataset" / "images" / "test"
    yolo_model_path = Path(__file__).parent / "models" / "best.pt"

    print(f"\nì´ë¯¸ì§€ ë””ë ‰í† ë¦¬: {image_dir}")
    print(f"YOLO ëª¨ë¸: {yolo_model_path}")

    # YOLO ë¡œë“œ
    print("\nYOLO ëª¨ë¸ ë¡œë”©...")
    yolo_detector = YOLODetector(str(yolo_model_path))
    print("âœ… YOLO ë¡œë“œ ì™„ë£Œ")

    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    results = {}

    # Test 1: AirLine ì•ˆì •ì„±
    results['airline'] = test_airline_stability(monitor, image_dir, yolo_detector, n_iterations=20)

    # Test 2: GPU ë©”ëª¨ë¦¬ í•œê³„
    results['gpu_memory'] = test_gpu_memory_limit(monitor)

    # Test 3: CRG311 ì•ˆì •ì„±
    results['crg311'] = test_crg311_stability(monitor, n_iterations=10)

    # Test 4: ë©”ëª¨ë¦¬ ì§‘ì•½ì  ì—°ì‚°
    results['memory_intensive'] = test_memory_intensive_ops(monitor)

    # ìµœì¢… ë¦¬í¬íŠ¸
    print("\n" + "="*60)
    print("ì§„ë‹¨ ê²°ê³¼ ìš”ì•½")
    print("="*60)

    for test_name, passed in results.items():
        status = "âœ… í†µê³¼" if passed else "âŒ ì‹¤íŒ¨"
        print(f"{test_name:20s}: {status}")

    # ê¶Œì¥ì‚¬í•­
    print("\n" + "="*60)
    print("ê¶Œì¥ì‚¬í•­")
    print("="*60)

    if not results['gpu_memory']:
        print("âš ï¸  GPU ë©”ëª¨ë¦¬ 80% ì œí•œ ì„¤ì • í•„ìš”")

    if not results['airline']:
        print("âš ï¸  AirLine ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°•í™” í•„ìš”")

    if not results['memory_intensive']:
        print("âš ï¸  ì£¼ê¸°ì  ë©”ëª¨ë¦¬ ì •ë¦¬ ê°•í™” í•„ìš” (5ë²ˆë§ˆë‹¤)")

    print("\në‹¤ìŒ ë‹¨ê³„: Phase 2 - GPU ë©”ëª¨ë¦¬ ê´€ë¦¬ ê°•í™”")
    print("="*60)


if __name__ == "__main__":
    main()
